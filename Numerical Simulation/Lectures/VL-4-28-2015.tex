\documentclass[../skript.tex]{subfiles}

\begin{document}

\subsection{General theorem on necessary and sufficient conditions}

Let $U$ be a Banach space, $f:\mathcal{D}(f)\subseteq U\to\R$. Consider \par

\begin{problem}
\label{prob:c1e4-star}
\begin{IEEEeqnarray*}{u"l"u}
	\text{minimize } & f(u),\\
	\text{subject to }&u\in N_j\subseteq U, \; j=1,\ldots,n & ``inequality constraints'' \\
	\text{and } & u\in M\subseteq U&``equality constraints''
\end{IEEEeqnarray*}
\end{problem}

\begin{definition} % Def 1.38
\label{def:c1e38}
	A vector $h\in U$ is called 
	\begin{enumerate}
		\item a \emph{regular descent direction} of $f$ at $u_0$, if there exist $t_0 > 0, \delta>0$ and a neighborhood $\mathcal{O}_0$ of zero, \ac{st}
			\[
				\frac{f(u_0+t(h+v)) - f(u_0)}{t}\leq -\delta,\quad\forall t\in (0,t_0),\forall v\in \mathcal{O}_0.
			\]
			The set of such $h$ is called the \emph{regular descent cone} of $f$ at $u_0$.
		\item an \emph{admissible direction} at $u_0$ \ac{wrt}\ $N\subseteq U$, if there exist $t_0>0$ and a neighborhood $\mathcal{O}_0$ of zero, \ac{st}
			\[
				u_0 + t(h+v)\in N,\quad \forall t\in (0,t_0),\forall v\in \mathcal{O}_0.
			\]
			The set of all these $h$ is called the \emph{admissible direction cone} at $u_0$ \ac{wrt}\ $N$.
	\end{enumerate}
\end{definition}
\begin{remarknonumb} 
	If $h$ is a regular descent direction, or an admissible direction, then $\alpha h$ is the same. So the nomenclature as `cones' makes sense. Moreover both cones are always open.
\end{remarknonumb}

Regarding \cref{prob:c1e4-star}, we now define
\begin{IEEEeqnarray*}{rCl}
	K_0 &\coloneqq& \text{regular descent cone at } u_0\text{ for }f \text{ in \cref{prob:c1e4-star}}\\
	K_j &\coloneqq& \text{admissible direction cones at }u_0\text{ \ac{wrt}\ }N_j,\,j=1,\ldots,n\\
	K_{n+1}&\coloneqq& \text{tangent cone to }M\text{ at }u_0\text{ \cref{def:c1e27} }
\end{IEEEeqnarray*}

\begin{theorem} % Thm 1.39
\label{thm:c1e39}
	For the \cref{prob:c1e4-star} we assume the following:
	\begin{enumerate}[(i)]
		\item \label[equation]{thm:c1e39:e1} $\Int{N_j}\not=\emptyset,\quad j=1,\ldots,n$,
		\item \label[equation]{thm:c1e39:e2} $K_0, \ldots ,K_{n+1}$ are convex, and $K_0\not=\emptyset$.
	\end{enumerate}
	Then, if $u_0$ is a local minimum for \cref{prob:c1e4-star}, there exist linear functions $\ell_i\in K_i^+,\,i=0, \ldots ,n+1$ which are not simultaneously zero, \ac{st} 
	\[
		\ell_0+\ell_1+ \ldots +\ell_{n+1} = 0\quad\text{`necessary condition'}.
	\]
	The $\ell_i$ are called \emph{generalized Lagrange-multipliers}. It holds that
	\[
		\ell_k\not=0 \quad \text{if }\bigcap_{\substack{i=0 \\i\neq k}}^{n+1} K_i\neq\emptyset.
	\]
The necessary condition condition (existence of the $\ell_i$) is sufficient for $u_0$ to be a global solution of \cref{prob:c1e4-star}, if additionally it holds that
\begin{enumerate}[(i)]
	\setcounter{enumi}{2}
	\item \label[equation]{thm:c1e39:e3}$f$ is convex and continuous,
	\item \label[equation]{thm:c1e39:e4}\emph{Slater condition}: $N_1, \ldots ,N_n$ and $M$ are convex and
			\[
				\left(\bigcap_{j=1}^n \Int\,N_j\right)\cap M\not=\emptyset.
			\]
\end{enumerate} 
\end{theorem}
\begin{remarknonumb}
	We can always assume that one $N_j$ is present, by choosing $N_j=U$.
\end{remarknonumb}

\begin{proof}
	We first prove that the condition is necessary.\par
	As we have stated in a remark above, $K_0,\ldots,K_n$ are open. We show that $\bigcap_{i=0}^{n+1}K_i=\emptyset$. Then the claim follows from the Dubovickii-Miljutin \cref{thm:c1e37}. Assume $h\in\bigcap_{i=0}^{n+1}K_i$. Then there exist $t_0>0,\delta>0$ and a neighborhood $\mathcal{O}_0$ of zero, s.t.
	\[
		\frac{f(u_0+  t(h+v)) - f(u_0)}{t}\leq -\delta 
	\]
	and
	\[
		u_0 + t(h+v)\in N_j,\quad j=1,\ldots,n,
	\]
	for all $t\in(0,t_0)$ and $v\in\mathcal{O}_0$, for some $t_0 > 0$. Further, let $\gamma$ be the admissible curve for $h\in K_{n+1}$, then for small enough $t$ we have
	\[
		\gamma(t) = \underbrace{u_0 + th + o(t)}_{=u_0 + t\left(h+\frac{\smallo(t)}{t}\right)}\in M, \; \text{ and } \; 0\leftarrow\frac{\smallo(t)}{t}\in\mathcal{O}_0.
	\]
	In other words:
	\[
		\gamma(t)\in N_1,\ldots,N_n,M\;\text{ and }\;f(\gamma(t)) - f(u_0) \leq -t\delta < 0,\text{ for small enough }t.
	\]
	As $\gamma(t)\to u_0,$ for $t\to 0$ this contradicts the local optimality of $u_0$.\par
	To prove the claim about $\ell_k\not=0$, assume, say, $\ell_0 = 0$, but $\bigcap_{i=1}^{n+1}K_i\not=\emptyset$. Then by assumption not all $\ell_1,\ldots,\ell_{n+1}$ vanish, so one can conclude from \cref{thm:c1e37} that $\bigcap_{i=1}^{n+1}K_i = \emptyset$, which is again a contradiction.\par
	We now prove the sufficient condition. We additionally assume the additional assumptions \labelcref{thm:c1e39:e3}, \labelcref{thm:c1e39:e4} hold, but that exists $u_1\in N_1\cap\ldots\cap N_n\cap M$ with $f(u_1)<f(u_0)$. By \labelcref{thm:c1e39:e4} we can pick $h\in \Int N_1 \cap\ldots\cap \Int N_n\cap M$. Let
	\[
		u_s = su_1 + (1-s)h,\quad 0<s<1.
	\]
	It follows that $u_s+v\in N_1\cap\ldots\cap N_n$ for small enough $\|v\|$ and by convexity
	\[
		u_0 + t(u_s+v-u_0)\in N_1\cap\ldots\cap N_n,\quad\forall t\in (0,1).
	\]
	By definition, this proves
	\[
		u_s-u_0\in K_j,\quad j=1,\ldots,n.
	\]
	Also, by convexity ($M$ is convex as well) we have 
	\[
		\underbrace{u_0 + t(u_s-u_0)}_{\gamma(t)}\in M,\quad\forall t\in(0,1).
	\]
	By definition, $u_s-u_0 \in K_{n+1}$. If we show that $u_s-u_0\in K_0$, so $u_s-u_0$ is also a descent direction, for some $s$, then we obtain a contradiction to \cref{thm:c1e37}. To show that $u_s-u_0\in K_0$, we choose $s\in(0,1)$ in a specific way, \ac{st}
	\[
		f(u_s+v) < f(u_0)\quad\text{ for small enough }\|v\|,
	\]
	which is possible by the continuity of $f$ and $f(u_s)\to f(u_1)$, for $s\to 1$. Since $f$ is assumed to be convex, so we get for these $v$ that
	\begin{IEEEeqnarray*}{rCl}
		f(u_0+t(u_s+v-u_0)) &\leq& t f(u_s+v) + (1-t) f(u_0), \\
		\noalign{or equivalently: \vspace{\jot}}	\frac{f(u_0+t(u_s-u_0+v))-f(u_0)}{t}&\leq& f(u_s+v)-f(u_0) < 0.
	\end{IEEEeqnarray*}
	Therefore $u_s-u_0$ is a regular descent direction, i.e.\ $u_s-u_0\in K_0$.
\end{proof}
The main difficulty is that the result is not applicable to point-wise constraints in $L_p$ spaces.
\begin{examplenumb} % Example 1.40
\label{ex:c1e40}
Consider the problem in $U = L_2[0,1]$:
\begin{IEEEeqnarray*}{u"l}
minimize & \| u - u_* \|_{L_2}^2, \; u_* \text{ given} \\
subject to & 0 \leq u(x) \leq 1 \text{ a.e.}
\end{IEEEeqnarray*}
This problem has a solution by \cref{thm:c1e14}, since the set $N = \{u\,:\,0\leq u(x)\leq 1\}$ is bounded and convex in $L_2[0,1]$. But $N$ has no interior point: Let
\[
	\varphi_N = \sqrt{n}\mathds{1}_{\left[0,\frac{1}{n^2}\right]},
\]
then 
\[
	\| u-(u+\varphi_n) \|_{L_2}^2 = \|\varphi_n\|_{L_2}^2 =\frac{1}{n},
\]
but $u+\varphi_n\not= N$. Therefore, the Lagrange-multiplier rule is not applicable. 

On the other hand, if we pose the problem in $C[0,1]$:
\begin{IEEEeqnarray*}{u"l}
minimize & \| u - u_* \|_\infty, \; u_* \in C[0,1] \text{ given} \\
subject to & 0 \leq u(x) \leq 1 \quad \forall x \in [0, 1]
\end{IEEEeqnarray*}
then $N = \{u\in C[0,1]\,:\, 0\leq u\leq 1\}$ \emph{does} contain an interior point and the \cref{thm:c1e39} is applicable. However, (\textbf{Exercise}):
\begin{enumerate}
\item Prove or disprove that the problem in $C[0,1]$ always has a solution.
\item Formulate the necessary conditions from \cref{thm:c1e39} (what is $C[0,1]^*$?).
\end{enumerate}
\end{examplenumb}
\end{document}