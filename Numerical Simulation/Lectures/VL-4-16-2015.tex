\documentclass[../skript.tex]{subfiles}

\begin{document}
\subsection{Application to quadratic variational problems}
Let $U$ be a real Banach space, and $a : U \times U \to \R$ a \emph{bilinear form}.
\begin{definition} % Definition 1.15
\label{def:c1e15}
The bilinear form $a$ is called
\begin{enumerate}[(i)]
\item \emph{bounded} if there exists $C > 0$ \ac{st} $a(u, v) \leq C \| u \| \| v \| \quad \forall u, v \in U$.
\item \emph{positive} (\ac{resp} strictly positive), if $a(u, u) \geq 0$ (\ac{resp} $>$) for all $u \in U$.
\item \emph{strongly positive} or coercive, if there exists $c > 0$, \ac{st}
\[
	a(u, u) \geq c \| u \|^2 \quad \forall u \in U.
\]
\end{enumerate}
\end{definition}
We consider a function
\[
	f(u) = \frac{1}{2} a(u, u) - b(u) + f(0)
\]
where $b \in U^*$ and $f(0)$ a constant.
\begin{proposition} % Prop 1.16
\label{prop:c1e16}
\begin{itemize}
\item If $a$ is bounded, then $f$ is continuous.
\item If $a$ is positive, then $f$ is convex.
\item If $a$ is strictly positive, then $f$ is strictly convex.
\item If $a$ is coercive, then $f$ is coercive.
\end{itemize}
\end{proposition}
\begin{proof}
Exercise.
\end{proof}
\begin{proposition} % Prop 1.17
\label{prop:c1e17}
Let $U$ be a real \emph{reflexive} Banach space and $C \subseteq U$ be closed and convex. Then the problem
\begin{mdframed}[style=theoremframing]
minimize $\frac{1}{2} a(u, u) - b(u) + f(0)$ ($a$ bounded) subject to $u \in C$.
\end{mdframed}
has at least one solution, if either
\begin{enumerate}[(i)]
\item \label[equation]{prop:c1e17-i} $C$ is bounded and $a$ is positive
\item \label[equation]{prop:c1e17-ii} $a$ is strongly positive (coercive)
\end{enumerate}
In case \labelcref{prop:c1e17-ii} the solution is unique. In case \labelcref{prop:c1e17-i} the solution is unique if $a$ is strictly positive.
\end{proposition}
\begin{proof}
\cref{thm:c1e14,prop:c1e16}
\end{proof}
\begin{theorem} % Thm 1.18
\label{thm:c1e18}
Let $(U, \| \cdot \|_U)$ and $(H, \| \cdot \|_H)$ be two real Hilbert spaces. Let $U_{ad} \subset U$ be closed and convex, $y_d \in H$ and $\lambda \geq 0$.
Further, let $S : U \to H$ be a bounded linear operator. Then the quadratic optimization problem\par
\begin{mdframed}[style=theoremframing]
minimize $f(u) = \frac{1}{2} \| Su - y_d \|_H^2 + \frac{\lambda}{2} \| u \|_U^2$ subject to $u \in U_{ad}$
\end{mdframed}
has a solution if one of the following three conditions is satisfied:
\begin{enumerate}[(i)]
\item \label[equation]{thm:c1e18-i} $U_{ad}$ is bounded
\item \label[equation]{thm:c1e18-ii} $\lambda > 0$
\item \label[equation]{thm:c1e18-iii} $S$ is bounded below: $\exists \sigma > 0$ \ac{st} $\| S u \| \geq \sigma \| u \|$.
\end{enumerate}
In cases \labelcref{thm:c1e18-ii}, \labelcref{thm:c1e18-iii} the solution is unique. In \labelcref{thm:c1e18-i} with $\lambda = 0$, the solution is unique (equivalent to $S$ is injective).
\end{theorem}
\begin{proof}
First of all, $U$ is reflexive since it is a Hilbert space. We write $f$ as
\[
	f(u) = \frac{1}{2} a(u, u) - b(u) + f(0)
\]
where $a(u, v) = \langle Su, Sv \rangle_H + \lambda \langle u, v \rangle_U$, $b(u) = \langle Su, y_d \rangle_H$., $f(0) = \frac{1}{2} \| y_d \|_H^2$.
Let $\sigma = \inf_{\| u \|_U = 1} \| S u \|_H$, then
\[
	a(u, u) \geq (\sigma^2 + \lambda \| u \|^2).
\]
Almost everything follows from \cref{prop:c1e17}. Uniqueness in case \labelcref{thm:c1e18-i} with $\lambda = 0$: If $S$ is injective, then $a$ is strictly positive. If $S$ is not injective, then $f(u) = f(u + h)$ for all $h$ from the null space of $S$.
Hence solutions cannot be unique.
\end{proof}
\begin{examplenumb}[linear best approximation] % Example 1.19
\label{ex:c1e19}
Let $H$ be a Hilbert space, and let $M \subseteq H$ be a closed subspace. Then for any $y_d \in H$ there exists a unique solution $\hat{y}_d$ of
\begin{mdframed}[style=theoremframing]
minimize $\| y - y_d \|$ \ac{st} $y \in M$.
\end{mdframed}
$\hat{y}_d$ is called \emph{best approximation}.
\end{examplenumb}
\pagebreak
\subsection{Fréchet differentiability}
\begin{definition} % Def 1.20
\label{def:c1e20}
Let $U$ and $V$ be Banach spaces. A function $f : U \to V$ is called \emph{Fréchet-differentiable} if there exists a bounded linear operator $f'(u) : U \to V$ \ac{st}
\[
	f(u +h) = f(u) + f'(u) h + \underbrace{ \smallo(\| h \|) }_{\lim_{h \to 0} \frac{o (\| h \|)}{\| h \|} = 0}.
\]
$f'(u)$ is called \emph{Fréchet derivative at $u$}.
If $f$ is Fréchet differentiable at every $u$, then $f$ is called \emph{Fréchet-differentiable}, and the map
\[
	f' : U \to \mathcal{L}(U, V) \; : \; u \mapsto f'(u)
\]
is the derivative.
If this map is continuous (\ac{wrt} to the operator norm in $\mathcal{L}(U, V)$), then $f$ is called \emph{continuously differentiable} or a \emph{$C^1$ function}.
\end{definition}
\underline{Recall:} $f$ is Fréchet differentiable implies $f$ is continuous.
In case $V = \R$, the derivatives $f'(u)$ are elements of $U^*$. In case of a Hilbert space, $U = U^*$.
\begin{definition} % Def 1.21
\label{def:c1e21}
Let $U$ be a real Hilbert space.
The identification of $f'(u)$ as an element of $U$ is called the \emph{gradient of $f$ in $U$}, and denoted by $\nabla f(u)$. By definition,
\[
	\langle \nabla f(u), h \rangle = f'(u)h \quad \forall h \in U.
\]
\end{definition}
\addtocounter{dummythm}{-1} % Correct num. error of the lecture
\begin{examplenumb} % Example 1.21
\label{ex:c1e21}
\begin{enumerate}[(i)]
\item\label{ex:c1e21-i} Let $A : U \to V$ be bounded and linear. Then $A$ is Fréchet-differentiable, and $A'(u) = A \quad \forall u \in U$.
\item\label{ex:c1e21-ii} \underline{Quadratic function on a Hilbert space:} \\
Let $U$ be a Hilbert space, $A \in \mathcal{L}(U, U)$ self-adjoint and $b \in U$. Then,
\[
	f(u) = \frac{1}{2} \langle u, A u \rangle - \langle b, u \rangle + f(0)
\]
is Fréchet-differentiable, and
\[
	\nabla f(u) = Au - b.
\]
\item\label{ex:c1e21-iii} Let $U = C[0, 1]$ and $f(u) = \sin(u(1))$.
\begin{IEEEeqnarray*}{rCl}
f(u + h) &=& \sin(u(1) + h(1)) \\
&=& \sin(u(1)) + \cos(u(1)) - h(1) - \underbrace{ \frac{1}{2} \sin(u(1)) [h(1)]^2 }_{\| \cdot \| \leq \frac{1}{2} \| h \|_\infty^2}
\end{IEEEeqnarray*}
Thus, $f'(u)h = \cos (u(1)) h(1)$.
\end{enumerate}
\end{examplenumb}
\begin{itemize}
\item \emph{Chain rule:} $(g \circ f)'(u) h = g'(f(u)) f'(u)h$
\item \emph{Partial derivatives:} $f : U_1 \times U_2 \to V$. Then
\[
	\nabla_{u_1} f(\bar{u}_1, \bar{u}_2) = \frac{\partial f}{\partial u_1}(\bar{u}_1, \bar{u}_2)
\]
is the Fréchet-derivative of
\[
	h_1 \mapsto f(\bar{u}_1 + h, \bar{u}_2)
\]
in zero. Similar for $u_2$.
\end{itemize}
\begin{proposition} % Prop 1.22
\label{prop:c1e22}
If $f : U_1 \times U_2 \to V$ is Fréchet-differentiable, then
\[
	f'(\bar{u}_1, \bar{u}_2)[u_1, u_2] = f_{u_1} (\bar{u}_1, \bar{u}_2)[h_1] + f_{u_2} (\bar{u}_1, \bar{u}_2)[h_2]
\]
In this case $f'(\bar{u}_1, \bar{u}_2) = 0$ \ac{iff} $f_{u_1}(\bar{u}_1, \bar{u}_2) = 0$ and $f_{u_2}(\bar{u}_1, \bar{u}_2) = 0$.
\end{proposition}
\subsection{First-order necessity conditions}
\begin{theorem} % Thm 1.23
\label{thm:c1e23}
Let $U$ be a real Banach space, $C \subseteq U$ and $f : \mathcal{U} \to \R$ be Fréchet-differentiable on an open set $\mathcal{U}$ which contains $C$. If $\bar{u} \in \Int C$ solves
\begin{mdframed}[style=theoremframing]
\begin{equation}
\label{eq:thm-c1e23-star}
\begin{IEEEeqnarraybox}{u"c}
minimize & f(u) \\
subect to & u \in C
\end{IEEEeqnarraybox} \tag{\ensuremath{\star}}
\end{equation}
\end{mdframed}
then $f'(\bar{u}) = 0$. If $f$ is convex, $f'(\bar{u}) = 0$ implies that $\bar{u}$ solves \labelcref{eq:thm-c1e23-star}.
\end{theorem}
\begin{theorem} % Thm 1.24
\label{thm:c1e24}
Consider the same situation as in \cref{thm:c1e23}, but assume that $C$ is closed and convex. Then, if $\bar{u} \in C$ (!) solves \labelcref{eq:thm-c1e23-star}, it holds
\[
	f'(u)[u - \bar{u}] \geq 0 \quad \forall u \in C \; \text{``\emph{varational inequality}''}.
\]
The converse is true, if $f$ is convex.
\end{theorem}
\begin{remarknonumb}
It is enough that $f$ is \emph{Gateaux-differentiable}.
\end{remarknonumb}
\begin{proof}
If $\bar{u}$ solves \labelcref{eq:thm-c1e23-star}, then
\[
f(\bar{u}) \leq f(\bar{u} + u - \bar{u}) = f(\bar{u}) + f'(\bar{u}[u - \bar{u}]) + \smallo(\| u - \bar{u} \|)
\]
Then,
\begin{IEEEeqnarray*}{rCl}
f'(\bar{u})\left[ \frac{u - \bar{u}}{ \| u - \bar{u} \|} \right] &=& \lim_{t \to 0^+} f'(\bar{u}) \left[ \frac{t (u - \bar{u})}{t \| u - \bar{u} \|} \right] \\
&\geq& \lim_{t \to 0^+} \frac{\smallo( t \| u - \bar{u} \| )}{t \| u - \bar{u} \| } = 0
\end{IEEEeqnarray*}
Thus,
\[
	f'(\bar{u})[u - \bar{u}] \geq 0.
\]
The converse follows from
\[
	f(u) - f(\bar{u}) \geq f'(u) [u - \bar{u}],
\]
which holds for convex $f$.
\end{proof}
\underline{Case of interest:}
\begin{mdframed}[style=theoremframing]
\begin{equation}
\label{eq:c1starstar}
\begin{IEEEeqnarraybox}{u"c}
minimize & \frac{1}{2} \| S u - y_d \|_H^2 + \frac{\lambda}{2} \| u \|_U^2 \\
subect to & u \in U_{ad}
\end{IEEEeqnarraybox} \tag{\ensuremath{\star\star}}
\end{equation}
\end{mdframed}
\begin{theorem} % Thm 1.25
\label{thm:c1e25}
Let $U$, $H$ be real Hilbert spaces, $S : U \to H$ be bounded, and $U_{ad} \subseteq U$ convex, $U_{ad} \neq \emptyset$. Then $\bar{u} \in U_{ad}$ solves \labelcref{eq:c1starstar} \ac{iff}
\[
\langle (S^* S + \lambda) u - S^* y_d, u - \bar{u} \rangle_U \geq 0 \quad \forall u \in U_{ad}.
\]
\end{theorem}
\begin{proof}
Write
\[
	f(u) = \frac{1}{2} \langle u, (S^* S + \lambda) u \rangle - \langle S^* y_d, u \rangle_U + \frac{1}{2} \| y_d \|_H^2.
\]
By \cref{ex:c1e21}, \labelcref{ex:c1e21-ii} it holds
\[
	\nabla f(u) = (S^* S + \lambda) u - S^* y_d.
\]
Now apply \cref{thm:c1e24}.
\end{proof}
\end{document}