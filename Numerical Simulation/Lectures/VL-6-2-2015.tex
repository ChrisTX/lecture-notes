\documentclass[../skript.tex]{subfiles}

\begin{document}
\section{Projected gradient method (PGM)} % 2.5
\label{sec:c2e5}
Let $f$ be a FrÃ©chet differentiable function, $U$ a real Hilbert space and $U_{ad}$ be a closed convex subset. Then we consider
\begin{IEEEeqnarray*}{u"l}
minimize & f(u) \\
subject to & u \in U_{ad} \subseteq U
\end{IEEEeqnarray*}
By the variational inequality \cref{thm:c1e25} we have the necessary condition for the minimizer $u_0 \in U_{ad}$ that
\begin{equation}
\label{eq:c2e5-star}
\opttag{$\star$}
\langle \nabla f(u_0), u - u_0 \rangle \geq 0 \quad \forall u \in U_{ad}
\end{equation}
\paragraph{Projection operators}
From functional analysis we know that for a closed, convex set $U_{ad}$ in a Hilbert space $U$ we have
\[
	\exists! \; \PP : U \to U_{ad} \;\; \text{\ac{st}} \;\; \| u - \PP (u) \|_{U} = \min_{u \in U_{ad}} \| u - v \|_U
\]
\begin{lemma}[Non-expansiveness of convex projections] % Lemma 2.26
\label{thm:c2e26}
It holds that
\[
	\| \PP(u) - \PP(v) \| \leq \| u - v \| \quad \forall u, v \in U.
\]
\end{lemma}
\begin{proof}
As $\PP(u)$ is a minimizer of
\[
	u \mapsto \frac{1}{2} \| v - u \|^2
\]
it satisfies
\[
	\langle \PP(v) - v, u - \PP(u) \rangle \geq 0 \quad \forall u \in U_{ad}.
\]
Adding two such inequalities for $v, \tilde{v}$ tested with $u = \PP(\tilde{v})$ and $u = \PP(v)$ gives
\begin{IEEEeqnarray*}{r"rCl}
& \langle \PP(v) - \PP(\tilde{v}) - (v - \tilde{v}), \PP(\tilde{v}) - \PP(v) \rangle &\geq& 0 \\
\Leftrightarrow & \langle \PP(v) - \PP(\tilde{v}), \PP(v) - \PP(\tilde{v}) \rangle &\leq& \langle v - \tilde{v}, \PP(v) - \PP(\tilde{v}) \rangle.
\end{IEEEeqnarray*}
The statement follows by Cauchy-Schwarz.
\end{proof}
\begin{lemma} % Lemma 2.27
\label{thm:c2e27}
For $\sigma > 0$, \cref{eq:c2e5-star} is equivalent to
\[
	u_0 = \PP(u_0 - \sigma \nabla f(u_0)) \quad \text{(fixed point equation)}.
\]
\end{lemma}
\begin{proof}
Let $u_0 \in U_{ad}$, \cref{eq:c2e5-star} holds. Then,
\begin{IEEEeqnarray*}{rCl}
	\| u_0 - \sigma \nabla f(u_0) - u \|_U^2 &=& \| u - u_0 \|_U^2 + 2 \sigma \langle \nabla f(u_0), u - u_0 \rangle + \sigma^2 \| \nabla f(u_0) \|_U^2 \\
	&\geq& \sigma^2 \| \nabla f(u_0) \|^2.
\end{IEEEeqnarray*}
Equality holds for $u = u_0$.
This shows that
\[
	\PP(u_0 - \sigma \nabla f(u_0)) = u_0.
\]
Conversely, if there exists $u \in U_{ad}$ such that
\[
	\langle \nabla f(u_0), u - u_0 \rangle < 0,
\]
then
\[
	\forall t \in [0, 1] \; : \; u_+ = u_0 + t(u - u_0) \in U_{ad}.
\]
Thus,
\[
	\| u_0 - \sigma \nabla f(u_0) - u_+ \|^2 = t^2 \| u - u_0 \|^2 + 2 \sigma t \underbrace{ \langle \nabla f(u_0), u - u_0 \rangle }_{{} < 0} + \sigma^2 \| \nabla f(u_0) \|_U^2
\]
is strictly less than $\sigma^2 \| \nabla f(u_0) \|^2$ for small enough $t$.
Therefore,
\[
	\PP(u_0 - \sigma \nabla f(u_0)) \neq u_0.
\]
\end{proof}
\paragraph{PGM:}
\[
	u^{(k+1)} \coloneqq \PP\left(u^{(k)} - \sigma \nabla f\left(u^{(k)}\right)\right).
\]
\begin{propositionnonumb}
If $f$ is strongly convex, i.e.
\[
	\exists \varrho > 0 \; : \; f(v) \geq f(u) + \langle \nabla f(u), v - u \rangle_U + \varrho \| u - v \|_U^2 \quad \forall v, u \in U
\]
then $u \to \nabla f(u)$ is strongly monotone, i.e.
\[
	\langle \nabla f(u) - \nabla f(v), u - v \rangle \geq 2 \varrho \| u - v \|^2.
\]
\end{propositionnonumb}
\begin{proof}
Definition of strong convexity and interchanging $u$ and $v$ and adding up the two inequalities.
\end{proof}
\begin{example}[strong convex functions]
\begin{enumerate}
\item Convex functions and regularization:
\[
	f(u) = \underbrace{ f_0(u) }_{\text{convex}} + \frac{\lambda}{2} \| u \|^2, \quad \lambda > 0.
\]
\item $C^2$-function with symmetric positive definite Hessian $A$:
\[
	f(v) = f(u) + \langle f(u), v - u \rangle + \frac{1}{2} \langle v - u, A(v - u) \rangle + \smallo\left(\| v - u \|^2 \right).
\]
\end{enumerate}
\end{example}
\begin{theorem}[convergence of PGM] % Theorem 2.28
\label{thm:c2e28}
Let $f$ be strongly convex and $\nabla f$ be Lipschitz-continuous on $U_{ad}$, i.e.
\[
\exists L > 0 \; : \; \| \nabla f(u) - \nabla f(v) \|_U \leq L \cdot \| u - v \|_U \quad \forall u, v \in U_{ad}.
\]
If $\sigma < \frac{2\varrho}{L^2}$, then for any $u^{(0)} \in U_{ad}$, the sequence $u^{(k)}$ generated by the PGM converges to the unique minimizer $u_0$ of $f$ on $U_{ad}$ at an (asymptotically) linear rate.
\end{theorem}
\begin{proof}
We show that
\[
F(u) \coloneqq \PP(u - \sigma \nabla f(u))
\]
is a contraction and then the result follows by Banach's fixed point theorem.
By \cref{thm:c2e26}, we have
\begin{IEEEeqnarray*}{rCl}
\| F(u) - F(v) \|^2 &\leq& \| u - \sigma \nabla f(u) - (v - \sigma \nabla f(v))\|^2 \\
&=& \| u - v \|^2 - 2 \sigma \underbrace{ \langle \nabla f(u) - \nabla f(v), u - v \rangle }_{{} \geq \varrho \| u - v \|^2} + \sigma^2 \underbrace{ \| \nabla f(u) - \nabla f(v) \|^2 }_{{} \leq L^2 \| u - v \|^2} \\
&\leq& (1 - 2 \sigma \varrho + \sigma^2 L^2) \| u - v \|^2
\end{IEEEeqnarray*}
The parabola defined by $\sigma \to 1 - \sigma \varrho + \sigma^2 L^2$ takes value $1$ in $\sigma = 0$ and $\sigma = \frac{2\varrho}{L^2}$. Hence, the parabola takes values smaller than $1$ between those values and we have a contraction.
\end{proof}
\begin{remarknonumb}
In practice one should choose the step sizes $\sigma_k$ adaptively,
\[
	u^{(k+1)} = \PP\left(u^{(k)} - \sigma_k \nabla f\left(u^{(k)}\right)\right).
\]
This can be done for instance by Armijo-line search.
\end{remarknonumb}
\begin{problemnonumb}[Optimal stationary heating]
\begin{IEEEeqnarray*}{u"l}
minimize & f(u) \coloneqq \frac{1}{2} \| Su - y_\Omega \|_{L_2}^2 + \frac{\lambda}{2} \| u \|_{L_2^2} \\
subject to & u \in U_{ad} \coloneqq \{ u \in L^2(\Omega) \mid u_a \leq u(x) \leq u_b \; \text{\ac{ae}}\}
\end{IEEEeqnarray*}
where $u_a u_b \in L_\infty(\Omega)$ and $Su \coloneqq y$ solves
\begin{IEEEeqnarray*}{rCl"l}
- \lapl y &=& \beta \cdot u & \text{in } \Omega \\
u &=& 0 & \text{on } \partial \Omega
\end{IEEEeqnarray*}
\end{problemnonumb}
Here it is easy to see that $\PP(u)$ is given point-wise:
\[
	\PP(u)(x) \coloneqq \PP_{[u_a(x), u_b(x)]} u(x).
\]
Moreover, we know from \cref{rem:c2e20} that
\[
	\nabla f(u) = \beta \cdot p + \lambda u
\]
with adjoint state $p$.
Choosing $\sigma = \frac{1}{\lambda}$ we get the PGM 
\[
	u^{(k+1)} \coloneqq \PP_{[u_a(x), u_b(x)]} \left( - \frac{1}{\lambda} \beta(x) p(x) \right)
\]
as already stated in \cref{thm:c2e24}.
Additionally, $f$ is strongly convex, so the convergence result applies.
In order to implement the PGM, we get the following scheme
\begin{enumerate}
\item Calculate state $y^{(k)}$ from control $u^{(k)}$ by solving
\[
	a(y^{(k)}, v) = \langle \beta u^{(k)}, v \rangle \quad \forall v \in V = H_0^1(\Omega),
\]
where $a$ is for example $S \nabla y^{(k)} \nabla v$.
\item Calculate the adjoint state $p^{(k)}$:
\[
	a(v, p^{(k)}) = \langle y^{(k)} - y_\Omega, v \rangle \quad \forall v\in V.
\]
\item Choose $\sigma$ and update
\[
	u^{(k+1)}(x) \coloneqq \PP_{[u_a(x), u_b(x)]} \left( u^{(k)}(x) - \sigma \left( \beta(x) p^{(k)} (x) + \lambda u^{(k)}(x) \right) \right)
\]
\end{enumerate}
\begin{lemma}[Regularity of optimal control] % Lemma 2.29
\label{thm:c2e29}
If $u_a, u_b \in H^1(\Omega)$, then the optimal control $\bar{u} \in H^1(\Omega)$.
\end{lemma}
\begin{proof}
Exercise (easy to see).
\end{proof}
\section{Full discretization of the model problem} % 2.6
\label{sec:c2e6}
In the full discussion of an optimal control problem \emph{all} quantities $y, p, u$ are replaced by discrete counterparts,
\[
	y_h \in V_h \subset V, \; p_h \in V_h, \; u_h \in U_h \subset U.
\]
Technically $U_h = U$ is possible, but we omit this case.
\begin{problemnonumb}[Discrete problem]
\begin{IEEEeqnarray*}{u"l}
minimize & f_h(u_h) \coloneqq \frac{1}{2} \| S_h u_h - y_{\Omega, h} \|_{L_2}^2 + \frac{\lambda}{2} \| u_h \|_{L_2}^2 \\
subject to & u_h \in U_{ad, h} \coloneqq \{ u \in U_h \mid u_a \leq u \leq u_b \; \text{\ac{ae}}\}
\end{IEEEeqnarray*}
Let $\lambda > 0$. Here, $S_h u_h = y_h$ solves
\[
	a(y_h, v_h) = \langle \beta u_h, v_h \rangle_{L_2} \quad \forall v_h \in V_h.
\]
\end{problemnonumb}
The task is exactly the same as before and all analogies hold (variational inequality, fixed point formulation, adjoint state, gradient formulation).
We now assume we can solve the discrete problem exactly for $\bar{u}_h$, e.g.\ by running the PGM.
The question is whether $\bar{u}_h \to \bar{u}$ for $h \to 0$ (and at what rate?).
\paragraph{Convergence for the stationary heat equation}
For simplicity, let $\beta \equiv 1$, $u_a \equiv \text{const}$ and $u_b \equiv \text{const}$.
Here, $V = H_0^1(\Omega)$, $U = L_2(\Omega)$ and
\[
	a(y, v) = \int_\Omega \nabla y \nabla v \dx.
\]
Discretization by FEM on a triangulation $\tau_h$ with elements $\Omega_j$:
\begin{IEEEeqnarray*}{rClCl}
	V_h &\coloneqq& \{ v_h \in C(\bar{\Omega}) &\mid& v_h|_{\Omega_j} \in P_1(\Omega_j) \;\; \forall \Omega_j \in \tau_h \} \\
	U_h &\coloneqq& \{ u_h \in L_2(\Omega) &\mid& u_h|_{\Omega_j} \in P_0(\Omega_j) \;\; \forall \Omega_j \in \tau_h \}
\end{IEEEeqnarray*}
Thus, functions in $V_h$ are piecewise linear (basis: hat functions on $\Omega_j$) and $U_h$ contains piecewise constant functions (basis: characteristic functions on $\Omega_j$).
The orthogonal projection $\Pi_h : U \to U_h$ is given by
\[
\left(\Pi_h u\right)(x) = \frac{1}{|\Omega_j|} \int_{\Omega_j} u(\xi) \dxi \quad \forall x \in \Omega_j.
\]
\begin{lemma} % Lemma 2.30
\label{thm:c2e30}
For box constraints $u_a \leq u(x) \leq u_b$ \ac{ae} it holds
\[
	u \in U_{ad} \implies \Pi_h u \in U_{ad, h}
\]
and
\[
	\exists c > 0 \; : \; \| \Pi_h u - u \|_{L_2} \leq c h^r \| u \|_{H^r(\Omega)}
\]
if $u \in H^r(\Omega)$ for arbitrary $r$.
\end{lemma}
\begin{proof}
Definition of box constraints and the Bramble-Hilbert Lemma.
\end{proof}
\begin{theorem} % Thm 2.31
\label{thm:c2e31}
For $h \to 0$, $u_a, u_b \in H^1(\Omega)$, the solution $\bar{u}_h \in U_{ad, h}$ of the discrete optimal control heating problem converges to the solution $u_0 \in U_{ad}$ of the continuous one.
If $\Omega$ is convex:
\[
\exists c > 0 \; : \; \| \bar{u}_h - \bar{u} \|_{L_2} \leq c \cdot h,
\]
where $h$ is the mesh width of the triangulation.
\end{theorem}
\end{document}