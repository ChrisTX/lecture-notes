\documentclass[a4paper]{amsart}

\usepackage{geometry}
\usepackage{parskip}
\usepackage{mathtools}
\usepackage{enumerate}

\usepackage{scrhack}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{mathtools}
\usepackage{IEEEtrantools}
\usepackage{nicefrac}
\usepackage[ngerman]{babel}
\usepackage{exscale}
\usepackage{stmaryrd}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{float}
\usepackage{IEEEtrantools}

\begin{document}
	\textbf{Exercise 1: }\par
	\textbf{(a) } Let $X$ real Hilbert space and $f(u) \coloneqq \|x\|^2_X$. Then we have
	\begin{eqnarray}
		f(u+h) = \|u+h\|^2_X 	&=& \langle u+h, u+h\rangle_X \nonumber\\ 
								&=& \langle u,u\rangle_X + 2\langle u,h\rangle_X + \langle h,h\rangle_X	\nonumber	
	\end{eqnarray}
	Define $f'(u) : X\to\mathbb{R}$ as $f'(u)h = 2\langle u,h\rangle_X$ which is obviously a \emph{bounded} and \emph{linear} operator. Moreover we have
	\[
		\frac{\langle h,h\rangle_X}{\|h\|_X} = \frac{\|h\|^2_X}{\|h\|_X} = \|h\|_X \to 0,\quad\text{for }\|h\|\to 0 
	\]
	Therefore we can write the above equation (using $\langle u,u\rangle_X = f(u)$) in the form
	\[
		 f(u+h) = f(u) + f'(u)h + o(\|h\|_X)
	\]
	and $f'(u)$ is the \emph{Fréchet derivative} of $f$, as $u\in X$ was arbitrary.\par

	\textbf{(b) } Let $X$ real Hilbert space, $A:X\to X$ self-adjoint, linear, bounded and $c\in\mathbb{R}$ constant. Define $f(u) \coloneqq \frac{1}{2}\langle u,Au\rangle_X + \langle b,u\rangle_X + c$. Then we have
	\begin{eqnarray*}
		f(u+h) 	&=& \frac{1}{2}\langle u+h,A(u+h)\rangle_X - \langle b,(u+h)\rangle_X + c\\
				&=& \frac{1}{2}\left[\langle u,Au\rangle_X+\langle u,Ah\rangle_X + \langle h,Au\rangle_X+\langle h,Ah\rangle_X\right] - \langle b,u\rangle_X - \langle b,h\rangle_X + c\\
				&=& \underbrace{\frac{1}{2}\langle u,Au\rangle_X - \langle b,u\rangle_X + c}_{=f(u)} + \frac{1}{2}\left[\langle u,Ah\rangle_X+\langle Au,h\rangle_X\right] - \langle b,h\rangle_X + \frac{1}{2}\langle h,h\rangle_X \\
				&\overset{A\,self-adjoint}=& f(u) + \langle Au,h\rangle_X + \langle b,h\rangle_X + \frac{1}{2}\langle h,Ah\rangle_X
	\end{eqnarray*}
	Define now $f'(u): X\to\mathbb{R}$ as 
	\[
		f'(u)h = \langle Au,h\rangle_X -\langle b,h\rangle_X,\quad\forall h\in X
	\]
	which is obviously a bounded linear operator. The last term in the above equation satisfies
	\[
		\frac{1}{2}\frac{\langle h,Ah\rangle_X}{\|h\|_X} \overset{C.S.\,ineq.}\leq \frac{1}{2}\frac{\|h\|_X\|Ah\|_X}{\|h\|_X} = \|Ah\|_X \to 0,\quad\text{ for }\|h\|\to 0
	\]
	therefore we can write
	\[
		f(u+h) = f(u) + f'(u)h + o(\|h\|_X)
	\]
	and $f'(u)$ is the Fréchet-derivative as $u\in X$ was arbitrary.\par

	\textbf{(c) } Let $X = C^1([0,1])$ and $L:[0,1]\times\mathbb{R}^n\times\mathbb{R}^n\to\mathbb{R}$ a $C^1$ function. Define
	\[
		f(u) \coloneqq \int_0^1 L(t,u(t),u'(t))\,dt
	\]
	As $L$ is a $C^1$ function, $L$ is also $C^1$ after fixing its second and third argument, so $L_{u,u'}:[0,1]\to\mathbb{R}$ is $C^1([0,1])$. Therefore it is continuous on $[0,1]$ which is a closed interval, so it is (using it maps to $\mathbb{R}$) \emph{Riemann-integrable} and we can express the integral e.g. as
	\begin{equation}\label{eqn_1}
		\int_0^1 L_{u,u'}(t)\,dt = sup\{U(Z)\vert \, Z\text{ decomposition of} [0,1]\}
	\end{equation}
	where 
	\[
		U(Z) = \sum_{k=1}^n \left[ (x_k-x_{k-1})\inf_{x_{k-1}<x<x_k} L_{u,u'}(t) \right]
	\]
	for a \emph{finite decomposition $Z$ of} $[0,1]$: $x_0=0 < x_1 < ... < x_n = 1$. \par
	As $L$ is $C^1$, we can now use the multidimensional Taylor-expansion, which leads to the definition of a Fréchet-derivative after reordering the terms occuring in equation~\ref{eqn_1}.\par\par

	\textbf{Exercise 2: } \par
	\textbf{$(i)\Rightarrow(ii)$ :} Assume that $A^Tx=b$ has a solution $x\in\mathbb{R}^m$ with $x\geq 0$. Let now $y\in\mathbb{R}^n$ with $Ay\geq 0$ arbitrary. Then we have
	\[
		b^T y = (A^Tx)\cdot y = \underbrace{x^T}_{\geq 0}\cdot\underbrace{Ay}_{\geq 0} \geq 0
	\] 
	This shows $(ii)$.\par\par
	\textbf{$(ii)\Rightarrow(i)$ :} For this direction we assume that $\forall y\in\mathbb{R}^n: Ay\geq 0 \Rightarrow b^T y\geq 0$. Assume that $(i)$ doesn't hold, i.e. $b\not\in P\coloneqq\{z = Ax,\,x\geq 0\}$, which is a \emph{convex cone}. Therefore we can find a hyperplane, which separates $P$ and $\{b\}$, namely we chose the hyperplane $y^Tz - d = 0$, where $z, d$ are chosen in a way that $P\in H_+$ (`positive' half-space), $b\in H_-$ ('negative' half-space). The half-spaces are meant to be half-spaces regarding the affine function $y^Tz-d = 0$. We can now chose (as $P$ is a convex cone through the origin) generating points $0, a_1, ...,a_k$ of $P$. For those points, as well as for arbitrary multiples of them, it holds that
	\begin{itemize}
		\item $-d\geq 0$
		\item $y^T\lambda a_i - d\geq 0\,\forall \lambda,\forall i=1,...,k$
		\item $y^T b < d$
	\end{itemize}

	which is a contradiction to $(ii)$. Therefore $(i)$ has to hold.\par\par


	\textbf{Exercise 3: } Let $X$ real Banach space, $M\subset X$ linear subspace and $b\in M$ fixed. Define $M^\perp \coloneqq\{f\in X^*\vert\, f(x) = 0,\forall x\in M\}$ and $B(M^\perp) \coloneqq \{ f\in M^\perp\vert\, \|f\| \leq 1\}$. We want to show that
	\[
		D\coloneqq \sup_{f\in B(M^\perp)} f(b)
	\]
	has a maximizer in $B(M^\perp)$ and that $D=\inf_{u\in M}\|b-u\|$.\par
	\textbf{Proof: } We first define a function $\tilde{E}: X^*\to\mathbb{R}$ by $\tilde{E}(f) = f(b),\,\forall f\in X^*$. Using the theory of the lecture, we could try to show that 
	\[
		minimize\,\tilde{E}(f) \text{ has a solution in } B(M^\perp) 
	\]
	As we want to find a maximizer, we reformulate our assumptions: \par
	Define $E:X^*\to\mathbb{R}$ as $E(f) = f(-b) \overset{f\in X^*}= -f(b),\,\forall f\in X^*$. As $M$ is a linear subspace of $X$, the choice of $b$ is equivalent to the choice of $-b$, so we can do that. We now want to use the theory from the lecture to show that $E$ has a minimizer in $B(M^\perp)$, which is equivalent to $\tilde{E}$ having the same element as a maximizer. The function $E$ is defined on $X^*\supset B(M^\perp)$. Moreover we have
	\[
		E(u+h) = (u+h)(b) \overset{u,h\in X^*}= u(b)+h(b) = E(u)+E(h)
	\]
	so we can define an operator
	\[
		E'(u): X^*\to\mathbb{R},\text{ by } h \mapsto E'(u)h = h(b)
	\]
	$E'$ is obviously linear and bounded, if we restrict it to $B(M^\perp)\subset X^*$. Therefore $E$ is Fréchet-differentiable on $B(M^\perp)$.
	Moreover, $E$ is convex:
	\begin{eqnarray*}
		E(\lambda f_1 + (1-\lambda)f_2) &=& (\lambda f_1 + (1-\lambda)f_2)(b) \\
		&\overset{\text{linear functionals}}=& \lambda f_1(b) + (1-\lambda)f_2(b)\\
		&=& \lambda E(f_1) + (1-\lambda)E(f_2),\quad\forall f_1,f_2\in X^*
	\end{eqnarray*}
	By Theorem 1.24 from the lecture we get now:
	\[
		E'(\bar{f}) = 0 \Longrightarrow \bar{f} \text{ minimizes }E \text{ on }B(M^\perp)
	\]
	This means we need to solve
	\[
		E'(\bar{f}) = 0
	\]
	on $B(M^\perp)$. Obviously we have the \emph{zero-functional} $\underline{0}\in B(M^\perp)$ and $E'(\underline{0}) = \underline{0}(b) = 0\in\mathbb{R}, \forall b\in M$. That means $0\in X^*$ \emph{minimizes} $E$ on $B(M^\perp)$, so it \emph{maximizes} $\tilde{E}$ on $B(M^\perp)$ and we have found our maximizor.\par
	Now we want to show that $D = \inf_{u\in M} \|b-u\|$. As $b\in M$ we already know that this infimum is attained for $u = b$ and yields the value $0$. Therefore we need to show that 
	\[
		\sup_{f\in B(M^\perp)} = \max_{f\in B(M^\perp)} \overset{\text{previous part}}= 0
	\]

	? ? ?
\end{document}